# backend/agent/__init__.py  â–¶ ìˆ˜ì •ë³¸ ì „ë¶€

from __future__ import annotations
import os, json, datetime as dt
from typing import List, Dict, Any, Literal
from zoneinfo import ZoneInfo

from sqlalchemy.orm import Session
from openai import OpenAI, AsyncOpenAI
import httpx

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationTokenBufferMemory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.agents import create_openai_tools_agent, AgentExecutor
from langchain.schema import SystemMessage
from langchain_core.tools import BaseTool
from langchain_core.tools import tool  # â¬…ï¸ ë°ì½”ë ˆì´í„°
from pydantic import BaseModel, Field

from .tools import make_toolset
from .planner import create_planner_prompt, plan_output_parser
from .executor import StepExecutor

import models
from routers.gcal import build_gcal_service
from routers.search import google_search_cse
from utils.image import fetch_and_resize

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ OpenAI í´ë¼ì´ì–¸íŠ¸
_sync_root = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    http_client=httpx.Client(
        timeout=30.0,
        limits=httpx.Limits(max_keepalive_connections=20),
    ),
)
_async_root = AsyncOpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    http_client=httpx.AsyncClient(
        timeout=30.0,
        limits=httpx.Limits(max_keepalive_connections=20),
    ),
)


class _SyncChat:
    def create(self, **kwargs):
        return _sync_root.chat.completions.create(**kwargs)


class _AsyncChat:
    async def create(self, **kwargs):
        return await _async_root.chat.completions.create(**kwargs)

# ì¼ë°˜ ì—ì´ì „íŠ¸ìš© LLM
_llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.2,
    client=_SyncChat(),
    async_client=_AsyncChat(),
)

# âœ… [ìˆ˜ì •] í”Œë˜ë„ˆ ì „ìš© LLMì„ ì—¬ê¸°ì„œ ì¤‘ì•™ ê´€ë¦¬í•©ë‹ˆë‹¤.
_planner_llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.2,
    client=_SyncChat(),      # ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ëœ í´ë¼ì´ì–¸íŠ¸ ì¬ì‚¬ìš©
    async_client=_AsyncChat(), # ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ëœ í´ë¼ì´ì–¸íŠ¸ ì¬ì‚¬ìš©
)

# â”€â”€ pydantic ìŠ¤í‚¤ë§ˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class CreateEventArgs(BaseModel):
    title: str  = Field(..., description="ì¼ì • ì œëª©")
    start: str  = Field(..., description="ISO-8601 ì‹œì‘")
    end:   str  = Field(..., description="ISO-8601 ì¢…ë£Œ")

class DeleteEventArgs(BaseModel):
    event_id: str

class WebSearchArgs(BaseModel):
    query: str
    k: int = 5

class GenImgArgs(BaseModel):
    prompt: str = Field(..., description="DALL-E í”„ë¡¬í”„íŠ¸")

class RecArgs(BaseModel):
    types: str
    limit: int = 5
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def tz_label(tz: dt.tzinfo) -> str:
    return getattr(tz, "key", None) or tz.tzname(None) or "UTC"

def _make_time_system_prompt(client_tz: ZoneInfo) -> str:
    now_local     = dt.datetime.now(client_tz)
    now_client    = now_local.isoformat(timespec="seconds")
    today_str     = now_local.date().isoformat()
    tomorrow_str  = (now_local.date() + dt.timedelta(days=1)).isoformat()

    # ì˜ˆì œ ì‹œê°„ (ë¯¸ë¦¬ ê³„ì‚°)
    afternoon_example = (now_local.replace(hour=13, minute=0, second=0) + 
                     dt.timedelta(days=(0 if now_local.hour < 13 else 1))).isoformat()

    system_prompt = (
        "You are an AI assistant that can also manage the user's Google Calendar.\n"
        f"â±ï¸ **Current client time ({tz_label(client_tz)}):** {now_client}\n\n"
        
        "## CALENDAR INSTRUCTIONS\n"
        f"- Today's date is {today_str} in timezone {tz_label(client_tz)}\n"
        f"- Tomorrow's date is {tomorrow_str}\n"
        f"- Current hour is {now_local.hour}\n"
        "- When user mentions 'ì˜¤ëŠ˜' (today), use today's date\n"
        "- When user mentions 'ë‚´ì¼' (tomorrow), use tomorrow's date\n"
        "- 'ì˜¤ì „' means AM, 'ì˜¤í›„' means PM\n"
        f"- Example: 'ì˜¤ëŠ˜ ì˜¤í›„ 1ì‹œ' should be interpreted as {afternoon_example}\n"
        "- ALWAYS create events in the FUTURE (after current time)\n"
        "- ALWAYS use the create_event tool for calendar requests\n\n"
        
        "## TOOL USAGE GUIDELINES\n"
        "- Use calendar tools (create_event, delete_event) when the user wants to manage their schedule\n"
        "- Use web_search when the user asks for current information or news\n"
        "- Use generate_image when the user asks to create or visualize an image\n\n"
        
        "## ABOUT THE RECOMMENDATION TOOL\n"
        "The fetch_recommendations tool should ONLY be used when:\n"
        "1. The user is EXPLICITLY asking for content suggestions, recommendations, or options\n"
        "2. The user is asking what to watch, read, or consume\n"
        "3. The user uses phrases like 'ì¶”ì²œí•´ì¤˜', 'ë­ ë³¼ê¹Œ?', 'ì–´ë–¤ ê²Œ ì¢‹ì„ê¹Œ?'\n\n"
        
        "NEVER use fetch_recommendations for:\n"
        "1. Technical explanations (e.g., 'Reactë€ ë¬´ì—‡ì¸ê°€?', 'TypeScriptì˜ ì¥ì ')\n"
        "2. Factual questions (e.g., 'íŒŒì´ì¬ í•¨ìˆ˜ ì •ì˜ ë°©ë²•', 'í•œêµ­ì˜ ìˆ˜ë„ëŠ”?')\n"
        "3. General conversation or advice\n\n"
        
        "If no tool is appropriate, just respond with a direct text answer. Most queries should be answered with text, not tools.\n"
    )
    print(system_prompt)
    return system_prompt

def format_tool_to_str(tool) -> str:
    """
    LangChain <0.3 ê³„ì—´ì— render util ì´ ì—†ì„ ë•Œ ì“°ëŠ” í´ë¦¬-í•„.
    LLM ì´ ì‰½ê²Œ ë”°ë¼ í•  ìˆ˜ ìˆë„ë¡ **JSON ì˜ˆì‹œ** ê¹Œì§€ ë„£ì–´ ì¤€ë‹¤.
    """
    fields = getattr(tool, "args_schema", None)
    fields   = getattr(tool, "args_schema", None)
    sample_args: dict[str, str] = {}
    if fields:
        for name, field in fields.__fields__.items():
            # pydantic v1 â†’ ModelField  /  v2 â†’ FieldInfo
            tp = (
                getattr(field, "annotation", None)   # v2
                or getattr(field, "outer_type_", None)  # v1
                or str
            )
            type_name = getattr(tp, "__name__", str(tp))
            sample_args[name] = f"<{type_name}>"
    return f"{tool.name} â€“ {tool.description or ''}"

def build_prompt(tools: list[BaseTool], tz: ZoneInfo) -> ChatPromptTemplate:
    tool_block  = "\n".join(format_tool_to_str(t) for t in tools)
    system_str  = _make_time_system_prompt(tz)

    # ì™„ì „íˆ í•˜ë“œì½”ë”©ëœ ì˜ˆì œ (ë¬¸ìì—´ í…œí”Œë¦¿ ë³€ìˆ˜ ì—†ìŒ)
    examples = """
    EXAMPLES:
    
    User: "TypeScriptë€ ë¬´ì—‡ì¸ê°€ìš”?"
    Assistant: TypeScriptëŠ” Microsoftì—ì„œ ê°œë°œí•œ JavaScriptì˜ ìƒìœ„ì§‘í•©(superset) í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤...
    
    User: "ì˜¤ëŠ˜ ì˜¤í›„ 1ì‹œì— íŒ€ íšŒì˜ ì¡ì•„ì¤˜"
    Assistant: {{"name": "create_event", "arguments": {{"title": "íŒ€ íšŒì˜", "start": "2025-05-26T13:00:00+02:00", "end": "2025-05-26T14:00:00+02:00"}}}}
    
    User: "ë‚´ì¼ ì ì‹¬ ì•½ì† ì¼ì • ì¶”ê°€í•´ì¤˜"
    Assistant: {{"name": "create_event", "arguments": {{"title": "ì ì‹¬ ì•½ì†", "start": "2025-05-27T12:00:00+02:00", "end": "2025-05-27T13:00:00+02:00"}}}}
    
    User: "ì˜í™” ì¶”ì²œí•´ì¤˜"
    Assistant: {{"name": "fetch_recommendations", "arguments": {{"types": "movie", "limit": 5}}}}
    
    User: "Reactì˜ ì¥ì ì´ ë­ì•¼?"
    Assistant: Reactì˜ ì£¼ìš” ì¥ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: 1) ê°€ìƒ DOMì„ í†µí•œ íš¨ìœ¨ì ì¸ ë Œë”ë§...
    """

    system_message = ("system",
             f"{system_str}\n\n"
             "You have access to the following tools:\n"
             f"{tool_block}\n"
             #f"{examples}\n\n"
             "When handling calendar requests:\n"
             "1. ALWAYS convert relative times to absolute ISO datetime\n"
             "2. ALWAYS check that the time is in the future\n"
             "3. ALWAYS include timezone information\n"
             "4. NEVER respond with natural language for calendar requests - use the tool\n\n"
             "When you need a tool, reply **only** with the JSON shown in the example -- "
             "no markdown, no extra keys, no natural-language."
            )
    print(system_message)

    return ChatPromptTemplate.from_messages(
        [
            system_message,
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
            ("assistant", "{agent_scratchpad}"),
        ]
    )

def _clean_history(messages: list[models.Message]) -> list[tuple[str,str]]:
    """ê¸°ëŠ¥-ì‘ë‹µ(âœ…/ğŸ—‘ï¸/â—/JSON) ì œê±° í›„ (role,text) íŠœí”Œ ë°˜í™˜"""
    cleaned = []
    for m in messages[-15:]:
        if m.role == "assistant":
            t = m.content.strip()
            if t.startswith(("âœ…", "ğŸ—‘ï¸", "â—", "ğŸ“·", '{"card_id', '{"prompt')):
                continue
        cleaned.append((m.role, m.content))
    return cleaned

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì—ì´ì „íŠ¸
def build_agent(
    db: Session,
    user: models.User,
    tz: ZoneInfo = ZoneInfo("UTC"),
    history: list[models.Message] | None = None,
) -> AgentExecutor:
    # 1) Memory â€“Â í† í° ê¸°ë°˜ ìœˆë„ìš° (â‰ˆ 1â€¯200â€¯tokens)
    memory = ConversationTokenBufferMemory(
        llm=_llm,
        memory_key="chat_history",
        input_key="input",
        max_token_limit=1200,
        return_messages=True,
    )
    for role, text in _clean_history(history or []):
        (memory.chat_memory.add_user_message if role == "user"
         else memory.chat_memory.add_ai_message)(text)

    # 2) tools & prompt
    tools  = make_toolset(db, user, tz, _sync_root, _llm)
    prompt = build_prompt(tools, tz)

    # 3) agent â†’ executor
    agent = create_openai_tools_agent(_llm, tools, prompt)
    exec_   = AgentExecutor(
        agent   = agent,
        tools   = tools,
        memory  = memory,
        verbose = True,  # ë””ë²„ê¹…ì„ ìœ„í•´ verbose ëª¨ë“œ í™œì„±í™”
        max_iterations      = 4,
        handle_parsing_errors = True,   # LLM ì´ JSON ê¹¨ëœ¨ë ¤ë„ í•œ ë²ˆ ë” ì‹œë„
        early_stopping_method = "force",  # ë” í™•ì‹¤í•œ ì œì–´
    )
    return exec_

# examples ë¶€ë¶„ì— ì¶”ê°€
#User: "ë‹¤ìŒ ë‹¬ ì£¼ë§ì— ë³¼ ë§Œí•œ ì „ì‹œíšŒ ì¶”ì²œí•˜ê³  ì¼ì • ì¡ì•„ì¤˜"
#Assistant (plan):
#{
# "steps":[
#   {"tool":"web_search","args":{"query":"ì„œìš¸ ì „ì‹œíšŒ 2025-08", "k":10}},
#   {"tool":"fetch_recommendations","args":{"types":"content","limit":5}},
#   {"tool":"create_event","args":{
#      "title":"ë°ì´ë¹„ë“œ í˜¸í¬ë‹ˆ ì „",
#      "start":"2025-08-16T14:00:00+09:00",
#      "end":"2025-08-16T16:00:00+09:00"}}
# ]
#}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ LCEL ê¸°ë°˜ Plan-and-Execute 1-íšŒ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_lcel_once(
    db: Session,
    user: models.User,
    tz: ZoneInfo,
    history: list[models.Message] | None = None,
    user_input: str | None = None,
) -> dict:
    """
    LLMì´ ê³„íšì„ ì„¸ìš°ê³ (Plan), ê° ë‹¨ê³„ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰(Execute)í•©ë‹ˆë‹¤.
    """
    # â”€â”€ 0) ì…ë ¥ í™•ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if user_input is None:
        if not history:
            raise ValueError("run_lcel_once: history or user_input is required.")
        
        last_user_message_content = None
        for message in reversed(history):
            if message.role == 'user':
                last_user_message_content = message.content
                break
        
        if last_user_message_content is None:
            return {"output": "ì´ì „ ëŒ€í™”ì—ì„œ ì‚¬ìš©ìë‹˜ì˜ ë©”ì‹œì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        user_input = last_user_message_content

    # â”€â”€ 1) í”Œë˜ë„ˆ í˜¸ì¶œ (ê³„íš ìˆ˜ë¦½) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # â€¼ï¸ [ìˆ˜ì •] í˜„ì¬ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ë™ì ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±
    now_in_client_tz = dt.datetime.now(tz)
    plan_prompt = create_planner_prompt(current_time_str=now_in_client_tz.isoformat())
    
    plan_chain = plan_prompt | _planner_llm | plan_output_parser

    print("\n" + "=" * 70)
    print(f"ğŸ•µï¸ 1. PLANNER INPUT: '{user_input}'")
    
    prompt_value = plan_prompt.invoke({"input": user_input})
    print("\n" + "-" * 25 + " ğŸ’Œ FINAL PROMPT TO LLM " + "-" * 25)
    for message in prompt_value.to_messages():
        print(f"[{message.type.upper()}]")
        print(message.content)
        print("---")
    print("-" * 75)

    # íŒŒì‹± ì „ LLM ì›ë³¸ ë‹µë³€ í™•ì¸
    raw_plan = (plan_prompt | _planner_llm).invoke({"input": user_input})
    print("\n" + "-" * 25 + " ğŸ¤– RAW LLM OUTPUT " + "-" * 26)
    print(raw_plan.content)
    print("-" * 75)

    try:
        plan = plan_output_parser.parse(raw_plan.content)
        print("\nğŸ“ 2. PARSED PLAN:\n", json.dumps(plan, indent=2, ensure_ascii=False))
    except Exception as e:
        print(f"\nâŒ ERROR: Failed to parse LLM output into JSON. Error: {e}")
        return {"output": "ì—ì´ì „íŠ¸ê°€ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (JSON íŒŒì‹± ì˜¤ë¥˜)"}

    # â”€â”€ 2) ë‹¨ê³„ë³„ ì‹¤í–‰ (Executor ì‚¬ìš©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    step_executor = StepExecutor(db, user, tz, _planner_llm, _sync_root)

    step_outputs: dict[str, str] = {}
    logs: list[dict] = []

    if not plan.get("steps"):
        print("\nğŸ¤· NO STEPS TO EXECUTE. Returning default response.")
    else:
        for idx, step in enumerate(plan.get("steps", [])):
            result = step_executor.execute_step(step, step_outputs)
            step_key = f"step_{idx + 1}_output"
            step_outputs[step_key] = result.get("output", "")
            logs.append(result)

    print("=" * 70 + "\n")

    # â”€â”€ 3) ìµœì¢… ì¶œë ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if logs:
        return {"output": logs[-1].get("output", "ì‹¤í–‰ì€ ì™„ë£Œë˜ì—ˆì§€ë§Œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")}
    
    return {"output": "ì•Œê² ìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?"}
